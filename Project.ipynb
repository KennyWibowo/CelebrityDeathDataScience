{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Final Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning/Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in our project is to import our dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will define a helper function that we will use to return the celebrity name and death date, along with any other features we would like to extract. \n",
    "\n",
    "If we pass in any page that isn't a wikipedia page of a deceased notable celebrity (as per our definition), the function will return `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_death_date(soup):\n",
    "    table_selector = soup.select('table.infobox.vcard')\n",
    "\n",
    "    if table_selector:\n",
    "        table = table_selector[0]\n",
    "        \n",
    "        # Important features we would like to handle specially\n",
    "        name = table.find('span', class_='fn')\n",
    "        born = table.find('span', class_='bday')\n",
    "        died = table.find('span', class_='dday deathdate')\n",
    "        \n",
    "        # Only return if the person has a name, has born data, and died\n",
    "        if name and born and died:\n",
    "            data =  {'Name': name.text, 'Born': born.text, 'Died': died.text}\n",
    "            \n",
    "            # Extract other features that we would like to keep.\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows:\n",
    "                header = row.find('th')\n",
    "                datum = row.find('td')\n",
    "                if header and datum and header.text not in data:\n",
    "                    data[header.text.replace('\\xa0', ' ').strip()] = datum.get_text(' ', strip=True)\n",
    "                    \n",
    "            return data\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will recursively traverse the links through Breadth First Search and visit all Wikipedia links reachable from the 'List of Celebrities' page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% - 0 left, 2966 recorded.                            \r"
     ]
    }
   ],
   "source": [
    "# Breadth First Search traversal.\n",
    "\n",
    "wikipedia_url = 'https://en.wikipedia.org'\n",
    "starting_endpoint = '/wiki/Lists_of_celebrities'\n",
    "click_limit = 2\n",
    "\n",
    "# Dictionary of visited links\n",
    "# 'url' -> number of clicks taken to get to link\n",
    "links_visited = {starting_endpoint: 0} \n",
    "# Queue of links to visit\n",
    "links_queue = deque()\n",
    "links_queue.appendleft(starting_endpoint)\n",
    "\n",
    "# Create a buffer of dataframes\n",
    "data_buffer = deque()\n",
    "\n",
    "# Debug info\n",
    "num_visited = 0\n",
    "\n",
    "# Find all links on the page that lead to another wikipedia page.\n",
    "while links_queue:\n",
    "    link = links_queue.pop()\n",
    "    page = requests.get(wikipedia_url + link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    num_visited += 1\n",
    "    \n",
    "    # Debug info\n",
    "    if links_visited[link] >= click_limit - 1:\n",
    "        num_links = len(links_visited)\n",
    "        num_deads = len(data_buffer)\n",
    "        percentage = format(100.0*(float(num_visited)/num_links), '.2f')\n",
    "        print((percentage + \"% - \" + str(len(links_queue)) + \" left, \" + str(num_deads) + \" recorded.\").ljust(60),  end='\\r', flush=True)\n",
    "    else:\n",
    "        print(\"Loading initial pages...\", end='\\r', flush=True)\n",
    "    \n",
    "    # See if page is of deceased person - if so, get the data\n",
    "    results = get_death_date(soup)\n",
    "    \n",
    "    # Append to dataframe buffer if we've found a match\n",
    "    if results:\n",
    "        data_buffer.append(pd.DataFrame([results]))\n",
    "    \n",
    "    # Break early on click limit or if page isn't a list\n",
    "    if links_visited[link] == click_limit or (\"List\" not in link and \"Category:\" not in link):\n",
    "        continue\n",
    "    \n",
    "    content = soup.find('div', id='content')\n",
    "    \n",
    "    # Continue BFS on links in the page\n",
    "    if content:\n",
    "        for candidate in content.findAll('a'):\n",
    "            if 'href' in candidate.attrs:\n",
    "                link_in_page = urlparse(candidate.attrs['href']).path\n",
    "                                \n",
    "                # Add to queue if it's a link we haven't seen before.\n",
    "                if link_in_page not in links_visited and link_in_page.startswith('/wiki/') and 'File' not in link_in_page:\n",
    "                    links_visited[link_in_page] = links_visited[link] + 1\n",
    "                    links_queue.appendleft(link_in_page)\n",
    "\n",
    "# Concatenate all of the buffer into the dataframe that holds our celebrity data\n",
    "df_celebrities = pd.concat(data_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this takes such a long time, we will run this once and cache it into a JSON file. All subsequent operations will be run off of the loaded JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('celebrities.json', 'w') as file:\n",
    "    file.write(df_celebrities.to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can read the file back in as the JSON in order to circumvent the crawling process for future usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_celebrities2 = pd.read_json(\"celebrities.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2966, 228)\n",
      "(2966, 228)\n"
     ]
    }
   ],
   "source": [
    "print(df_celebrities2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
