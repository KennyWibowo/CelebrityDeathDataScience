{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Final Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning/Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in our project is to import our dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from collections import deque\n",
    "from pytrends.request import TrendReq\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytrends in /Users/joseph/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: lxml in /Users/joseph/anaconda3/lib/python3.6/site-packages (from pytrends)\n",
      "Requirement already satisfied: requests in /Users/joseph/anaconda3/lib/python3.6/site-packages (from pytrends)\n",
      "Requirement already satisfied: pandas in /Users/joseph/anaconda3/lib/python3.6/site-packages (from pytrends)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/joseph/anaconda3/lib/python3.6/site-packages (from requests->pytrends)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/joseph/anaconda3/lib/python3.6/site-packages (from requests->pytrends)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Users/joseph/anaconda3/lib/python3.6/site-packages (from requests->pytrends)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/joseph/anaconda3/lib/python3.6/site-packages (from requests->pytrends)\n",
      "Requirement already satisfied: python-dateutil>=2 in /Users/joseph/anaconda3/lib/python3.6/site-packages (from pandas->pytrends)\n",
      "Requirement already satisfied: pytz>=2011k in /Users/joseph/anaconda3/lib/python3.6/site-packages (from pandas->pytrends)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /Users/joseph/anaconda3/lib/python3.6/site-packages (from pandas->pytrends)\n",
      "Requirement already satisfied: six>=1.5 in /Users/joseph/anaconda3/lib/python3.6/site-packages (from python-dateutil>=2->pandas->pytrends)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 9.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pytrends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will define a helper function that we will use to return the celebrity name and death date, along with any other features we would like to extract. \n",
    "\n",
    "If we pass in any page that isn't a wikipedia page of a deceased notable celebrity (as per our definition), the function will return `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_death_date(soup):\n",
    "    table_selector = soup.select('table.infobox.vcard')\n",
    "\n",
    "    if table_selector:\n",
    "        table = table_selector[0]\n",
    "        \n",
    "        # Important features we would like to handle specially\n",
    "        name = table.find('span', class_='fn')\n",
    "        born = table.find('span', class_='bday')\n",
    "        died = table.find('span', class_='dday deathdate')\n",
    "        \n",
    "        # Only return if the person has a name, has born data, and died\n",
    "        if name and born and died:\n",
    "            data =  {'Name': name.text, 'Born': born.text, 'Died': died.text}\n",
    "            \n",
    "            # Extract other features that we would like to keep.\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows:\n",
    "                header = row.find('th')\n",
    "                datum = row.find('td')\n",
    "                if header and datum and header.text not in data:\n",
    "                    data[header.text.replace('\\xa0', ' ').strip()] = datum.get_text(' ', strip=True)\n",
    "                    \n",
    "            return data\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will recursively traverse the links through Breadth First Search and visit all Wikipedia links reachable from the 'List of Celebrities' page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% - 0 left, 2966 recorded.                            \r"
     ]
    }
   ],
   "source": [
    "# Breadth First Search traversal.\n",
    "\n",
    "wikipedia_url = 'https://en.wikipedia.org'\n",
    "starting_endpoint = '/wiki/Lists_of_celebrities'\n",
    "click_limit = 2\n",
    "\n",
    "# Dictionary of visited links\n",
    "# 'url' -> number of clicks taken to get to link\n",
    "links_visited = {starting_endpoint: 0} \n",
    "# Queue of links to visit\n",
    "links_queue = deque()\n",
    "links_queue.appendleft(starting_endpoint)\n",
    "\n",
    "# Create a buffer of dataframes\n",
    "data_buffer = deque()\n",
    "\n",
    "# Debug info\n",
    "num_visited = 0\n",
    "\n",
    "# Find all links on the page that lead to another wikipedia page.\n",
    "while links_queue:\n",
    "    link = links_queue.pop()\n",
    "    page = requests.get(wikipedia_url + link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    num_visited += 1\n",
    "    \n",
    "    # Debug info\n",
    "    if links_visited[link] >= click_limit - 1:\n",
    "        num_links = len(links_visited)\n",
    "        num_deads = len(data_buffer)\n",
    "        percentage = format(100.0*(float(num_visited)/num_links), '.2f')\n",
    "        print((percentage + \"% - \" + str(len(links_queue)) + \" left, \" + str(num_deads) + \" recorded.\").ljust(60),  end='\\r', flush=True)\n",
    "    else:\n",
    "        print(\"Loading initial pages...\", end='\\r', flush=True)\n",
    "    \n",
    "    # See if page is of deceased person - if so, get the data\n",
    "    results = get_death_date(soup)\n",
    "    \n",
    "    # Append to dataframe buffer if we've found a match\n",
    "    if results:\n",
    "        data_buffer.append(pd.DataFrame([results]))\n",
    "    \n",
    "    # Break early on click limit or if page isn't a list\n",
    "    if links_visited[link] == click_limit or (\"List\" not in link and \"Category:\" not in link):\n",
    "        continue\n",
    "    \n",
    "    content = soup.find('div', id='content')\n",
    "    \n",
    "    # Continue BFS on links in the page\n",
    "    if content:\n",
    "        for candidate in content.findAll('a'):\n",
    "            if 'href' in candidate.attrs:\n",
    "                link_in_page = urlparse(candidate.attrs['href']).path\n",
    "                                \n",
    "                # Add to queue if it's a link we haven't seen before.\n",
    "                if link_in_page not in links_visited and link_in_page.startswith('/wiki/') and 'File' not in link_in_page:\n",
    "                    links_visited[link_in_page] = links_visited[link] + 1\n",
    "                    links_queue.appendleft(link_in_page)\n",
    "\n",
    "# Concatenate all of the buffer into the dataframe that holds our celebrity data\n",
    "df_celebrities = pd.concat(data_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this takes such a long time, we will run this once and cache it into a JSON file. All subsequent operations will be run off of the loaded JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('celebrities.json', 'w') as file:\n",
    "    file.write(df_celebrities.to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can read the file back in as the JSON in order to circumvent the crawling process for future usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_celebrities = pd.read_json(\"celebrities.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_objects = []\n",
    "\n",
    "base_date = datetime.strptime(\"2004-01-01\", \"%Y-%m-%d\")\n",
    "\n",
    "for date in df_celebrities[\"Died\"]:\n",
    "    date_objects.append(datetime.strptime(date, \"%Y-%m-%d\"))\n",
    "df_celebrities[\"Died\"] = date_objects\n",
    "\n",
    "\n",
    "df_valid = df_celebrities[df_celebrities[\"Died\"] >= base_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since pytrends records begin in 2004, we will discard any deaths that happened before that. This leaves us with 846 celebrities that died from 2004-01-01 to present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Celebrity trends parsed = 846\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#Search word to compare it to\n",
    "comparator = \"Grand Canyon\"\n",
    "\n",
    "#Open the pytrends request\n",
    "pytrends = TrendReq(hl='en-US', tz=360)\n",
    "\n",
    "#Create a list for the new column in the table\n",
    "trends = []\n",
    "\n",
    "#Find the relative trend at time of death compared to comparator\n",
    "#Comparator relative trend value taken from 2004-01-01\n",
    "def relative_trend(name, died):\n",
    "    #Enter the name for searching the trend\n",
    "    kw_list = [comparator, name]\n",
    "    #Get a dataframe of the relative trends\n",
    "    e_catcher = 0\n",
    "    while e_catcher == 0:\n",
    "        try:\n",
    "            pytrends.build_payload(kw_list, timeframe='all')\n",
    "            df_py = pytrends.interest_over_time()\n",
    "            e_catcher = 1\n",
    "        except:\n",
    "            time.sleep(3600)\n",
    "    comparator_value = df_py[comparator].iloc[0]\n",
    "    return df_py[(df_py.index.month == died.month) & (df_py.index.year == died.year)][name].iloc[0] - comparator_value\n",
    "    \n",
    "\n",
    "#Loop through and find all the trends relative to grand canyon\n",
    "counter = 0\n",
    "for name, died in zip(df_valid[\"Name\"], df_valid[\"Died\"]):\n",
    "    #Check if there are more than two parts to the name\n",
    "    name_split = name.split(\" \")\n",
    "    name_split_len = len(name_split)\n",
    "    shortened_name = name_split[0] + \" \" + name_split[name_split_len-1]\n",
    "    if  name_split_len > 2:\n",
    "        long_rt = relative_trend(name,died)\n",
    "        short_rt = relative_trend(shortened_name,died)\n",
    "        trends.append(max(long_rt, short_rt))\n",
    "    else:\n",
    "        trends.append(relative_trend(name,died))\n",
    "    counter += 1\n",
    "    print((\"Celebrity trends parsed = \" + str(counter)), end='\\r', flush=True)\n",
    "df_valid['Trends'] = trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('valid.json', 'w') as file:\n",
    "    file.write(df_valid.to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
