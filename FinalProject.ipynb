{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Final Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning/Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in our project is to import our dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from collections import deque\n",
    "from pytrends.request import TrendReq\n",
    "from datetime import datetime\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytrends in /Users/joseph/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: lxml in /Users/joseph/anaconda3/lib/python3.6/site-packages (from pytrends)\n",
      "Requirement already satisfied: requests in /Users/joseph/anaconda3/lib/python3.6/site-packages (from pytrends)\n",
      "Requirement already satisfied: pandas in /Users/joseph/anaconda3/lib/python3.6/site-packages (from pytrends)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/joseph/anaconda3/lib/python3.6/site-packages (from requests->pytrends)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/joseph/anaconda3/lib/python3.6/site-packages (from requests->pytrends)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Users/joseph/anaconda3/lib/python3.6/site-packages (from requests->pytrends)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/joseph/anaconda3/lib/python3.6/site-packages (from requests->pytrends)\n",
      "Requirement already satisfied: python-dateutil>=2 in /Users/joseph/anaconda3/lib/python3.6/site-packages (from pandas->pytrends)\n",
      "Requirement already satisfied: pytz>=2011k in /Users/joseph/anaconda3/lib/python3.6/site-packages (from pandas->pytrends)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /Users/joseph/anaconda3/lib/python3.6/site-packages (from pandas->pytrends)\n",
      "Requirement already satisfied: six>=1.5 in /Users/joseph/anaconda3/lib/python3.6/site-packages (from python-dateutil>=2->pandas->pytrends)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 9.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pytrends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will define a helper function that we will use to return the celebrity name and death date, along with any other features we would like to extract. \n",
    "\n",
    "If we pass in any page that isn't a wikipedia page of a deceased notable celebrity (as per our definition), the function will return `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_death_date(soup):\n",
    "    table_selector = soup.select('table.infobox.vcard')\n",
    "\n",
    "    if table_selector:\n",
    "        table = table_selector[0]\n",
    "        \n",
    "        # Important features we would like to handle specially\n",
    "        name = table.find('span', class_='fn')\n",
    "        born = table.find('span', class_='bday')\n",
    "        died = table.find('span', class_='dday deathdate')\n",
    "        \n",
    "        # Only return if the person has a name, has born data, and died\n",
    "        if name and born and died:\n",
    "            data =  {'Name': name.text, 'Born': born.text, 'Died': died.text}\n",
    "            \n",
    "            # Extract other features that we would like to keep.\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows:\n",
    "                header = row.find('th')\n",
    "                datum = row.find('td')\n",
    "                if header and datum and header.text not in data:\n",
    "                    data[header.text.replace('\\xa0', ' ').strip()] = datum.get_text(' ', strip=True)\n",
    "                    \n",
    "            return data\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will recursively traverse the links through Breadth First Search and visit all Wikipedia links reachable from the 'List of Celebrities' page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% - 0 left, 2966 recorded.                            \r"
     ]
    }
   ],
   "source": [
    "# Breadth First Search traversal.\n",
    "\n",
    "wikipedia_url = 'https://en.wikipedia.org'\n",
    "starting_endpoint = '/wiki/Lists_of_celebrities'\n",
    "click_limit = 2\n",
    "\n",
    "# Dictionary of visited links\n",
    "# 'url' -> number of clicks taken to get to link\n",
    "links_visited = {starting_endpoint: 0} \n",
    "# Queue of links to visit\n",
    "links_queue = deque()\n",
    "links_queue.appendleft(starting_endpoint)\n",
    "\n",
    "# Create a buffer of dataframes\n",
    "data_buffer = deque()\n",
    "\n",
    "# Debug info\n",
    "num_visited = 0\n",
    "\n",
    "# Find all links on the page that lead to another wikipedia page.\n",
    "while links_queue:\n",
    "    link = links_queue.pop()\n",
    "    page = requests.get(wikipedia_url + link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    num_visited += 1\n",
    "    \n",
    "    # Debug info\n",
    "    if links_visited[link] >= click_limit - 1:\n",
    "        num_links = len(links_visited)\n",
    "        num_deads = len(data_buffer)\n",
    "        percentage = format(100.0*(float(num_visited)/num_links), '.2f')\n",
    "        print((percentage + \"% - \" + str(len(links_queue)) + \" left, \" + str(num_deads) + \" recorded.\").ljust(60),  end='\\r', flush=True)\n",
    "    else:\n",
    "        print(\"Loading initial pages...\", end='\\r', flush=True)\n",
    "    \n",
    "    # See if page is of deceased person - if so, get the data\n",
    "    results = get_death_date(soup)\n",
    "    \n",
    "    # Append to dataframe buffer if we've found a match\n",
    "    if results:\n",
    "        data_buffer.append(pd.DataFrame([results]))\n",
    "    \n",
    "    # Break early on click limit or if page isn't a list\n",
    "    if links_visited[link] == click_limit or (\"List\" not in link and \"Category:\" not in link):\n",
    "        continue\n",
    "    \n",
    "    content = soup.find('div', id='content')\n",
    "    \n",
    "    # Continue BFS on links in the page\n",
    "    if content:\n",
    "        for candidate in content.findAll('a'):\n",
    "            if 'href' in candidate.attrs:\n",
    "                link_in_page = urlparse(candidate.attrs['href']).path\n",
    "                                \n",
    "                # Add to queue if it's a link we haven't seen before.\n",
    "                if link_in_page not in links_visited and link_in_page.startswith('/wiki/') and 'File' not in link_in_page:\n",
    "                    links_visited[link_in_page] = links_visited[link] + 1\n",
    "                    links_queue.appendleft(link_in_page)\n",
    "\n",
    "# Concatenate all of the buffer into the dataframe that holds our celebrity data\n",
    "df_celebrities = pd.concat(data_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this takes such a long time, we will run this once and cache it into a JSON file. All subsequent operations will be run off of the loaded JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_celebrities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-e90a05a94c5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'celebrities.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_celebrities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'records'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_celebrities' is not defined"
     ]
    }
   ],
   "source": [
    "with open('celebrities.json', 'w') as file:\n",
    "    file.write(df_celebrities.to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can read the file back in as the JSON in order to circumvent the crawling process for future usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_celebrities = pd.read_json(\"celebrities.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_objects = []\n",
    "\n",
    "base_date = datetime.strptime(\"2004-01-01\", \"%Y-%m-%d\")\n",
    "\n",
    "for date in df_celebrities[\"Died\"]:\n",
    "    date_objects.append(datetime.strptime(date, \"%Y-%m-%d\"))\n",
    "df_celebrities[\"Died\"] = date_objects\n",
    "\n",
    "\n",
    "df_valid = df_celebrities[df_celebrities[\"Died\"] >= base_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since pytrends records begin in 2004, we will discard any deaths that happened before that. This leaves us with 846 celebrities that died from 2004-01-01 to present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Celebrity trends parsed = 846\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#Search word to compare it to\n",
    "comparator = \"Grand Canyon\"\n",
    "\n",
    "#Open the pytrends request\n",
    "pytrends = TrendReq(hl='en-US', tz=360)\n",
    "\n",
    "#Create a list for the new column in the table\n",
    "trends = []\n",
    "comparator_average_trends = []\n",
    "trend_ratio = []\n",
    "\n",
    "#Find the relative trend at time of death compared to comparator\n",
    "#Comparator relative trend value taken from 2004-01-01\n",
    "def relative_trend(name, died):\n",
    "    #Enter the name for searching the trend\n",
    "    kw_list = [comparator, name]\n",
    "    #Get a dataframe of the relative trends\n",
    "    e_catcher = 0\n",
    "    while e_catcher == 0:\n",
    "        try:\n",
    "            pytrends.build_payload(kw_list, timeframe='all')\n",
    "            df_py = pytrends.interest_over_time()\n",
    "            e_catcher = 1\n",
    "        except:\n",
    "            time.sleep(3600)\n",
    "    cv = df_py[comparator].tolist()\n",
    "    comparator_avg = sum(cv)/float(len(cv))\n",
    "    trend = df_py[(df_py.index.month == died.month) & (df_py.index.year == died.year)][name].iloc[0]\n",
    "    trend_ratio = trend/comparator_avg\n",
    "    return trend, comparator_avg, trend_ratio\n",
    "    \n",
    "\n",
    "#Loop through and find all the trends relative to grand canyon\n",
    "counter = 0\n",
    "for name, died in zip(df_valid[\"Name\"], df_valid[\"Died\"]):\n",
    "    #Check if there are more than two parts to the name\n",
    "    name_split = name.split(\" \")\n",
    "    name_split_len = len(name_split)\n",
    "    shortened_name = name_split[0] + \" \" + name_split[name_split_len-1]\n",
    "    if  name_split_len > 2:\n",
    "        long_rt, cvl, trl = relative_trend(name,died)\n",
    "        short_rt, cvs, trs = relative_trend(shortened_name,died)\n",
    "        if long_rt > short_rt:\n",
    "            trends.append(long_rt)\n",
    "            comparator_average_trends.append(cvl)\n",
    "            trend_ratio.append(trl)\n",
    "        else:\n",
    "            trends.append(short_rt)\n",
    "            comparator_average_trends.append(cvs)\n",
    "            trend_ratio.append(trs)\n",
    "    else:\n",
    "        trend, cv, tr = relative_trend(name,died)\n",
    "        trends.append(trend)\n",
    "        comparator_average_trends.append(cv)\n",
    "        trend_ratio.append(tr)\n",
    "    counter += 1\n",
    "    print((\"Celebrity trends parsed = \" + str(counter)), end='\\r', flush=True)\n",
    "df_valid['Trends'] = trends\n",
    "df_valid['Comp_Average'] = comparator_average_trends\n",
    "df_valid['Trend_Ratio'] = trend_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('valid.json', 'w') as file:\n",
    "    file.write(df_valid.to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.read_json(\"valid.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid[\"Died\"] = pd.to_datetime(df_valid['Died'], unit=\"ms\")\n",
    "#Get rid of 2018, since its not a complete year\n",
    "df_valid = df_valid[df_valid[\"Died\"] < datetime.strptime(\"2018-01-01\", \"%Y-%m-%d\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of deaths per year\n",
    "deaths_per_year = defaultdict(int)\n",
    "for row in df_valid[\"Died\"]:\n",
    "    deaths_per_year[row.year] += 1\n",
    "deaths_sorted = OrderedDict(sorted(deaths_per_year.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deaths in 2004 = 56\n",
      "Deaths in 2005 = 48\n",
      "Deaths in 2006 = 56\n",
      "Deaths in 2007 = 48\n",
      "Deaths in 2008 = 58\n",
      "Deaths in 2009 = 45\n",
      "Deaths in 2010 = 62\n",
      "Deaths in 2011 = 61\n",
      "Deaths in 2012 = 58\n",
      "Deaths in 2013 = 61\n",
      "Deaths in 2014 = 60\n",
      "Deaths in 2015 = 63\n",
      "Deaths in 2016 = 78\n",
      "Deaths in 2017 = 77\n"
     ]
    }
   ],
   "source": [
    "#Print number of deaths per year sorted by deaths\n",
    "for key,value in deaths_sorted.items():\n",
    "    print( \"Deaths in \" + str(key) + \" = \" + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the trends per year\n",
    "trends_per_year = defaultdict(list)\n",
    "for key,value in deaths_per_year.items():\n",
    "    df = df_valid[(df_valid.Died >= datetime.strptime(str(key), \"%Y\")) & (df_valid.Died < datetime.strptime(str(key + 1), \"%Y\"))]\n",
    "    trends_per_year[key] = df.Trends.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the sum of trends and average of trends per year\n",
    "trends_per_year_sum = defaultdict(int)\n",
    "trends_per_year_avg = defaultdict(float)\n",
    "\n",
    "for key,value in trends_per_year.items():\n",
    "    tr = trends_per_year[key]\n",
    "    trends_per_year_sum[key] = sum(tr)\n",
    "    trends_per_year_avg[key] = sum(tr)/ float(len(tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_per_year_sum_ord = OrderedDict(sorted(trends_per_year_sum.items()))\n",
    "trends_per_year_avg_ord = OrderedDict(sorted(trends_per_year_avg.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend sum in 2004 = -2862\n",
      "Trend sum in 2005 = -2604\n",
      "Trend sum in 2006 = -3329\n",
      "Trend sum in 2007 = -2884\n",
      "Trend sum in 2008 = -3060\n",
      "Trend sum in 2009 = -2069\n",
      "Trend sum in 2010 = -2992\n",
      "Trend sum in 2011 = -3044\n",
      "Trend sum in 2012 = -2297\n",
      "Trend sum in 2013 = -3297\n",
      "Trend sum in 2014 = -2630\n",
      "Trend sum in 2015 = -3084\n",
      "Trend sum in 2016 = -3120\n",
      "Trend sum in 2017 = -3632\n"
     ]
    }
   ],
   "source": [
    "for key,value in trends_per_year_sum_ord.items():\n",
    "    print( \"Trend sum in \" + str(key) + \" = \" + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend avg in 2004 = -51.107142857142854\n",
      "Trend avg in 2005 = -54.25\n",
      "Trend avg in 2006 = -59.44642857142857\n",
      "Trend avg in 2007 = -60.083333333333336\n",
      "Trend avg in 2008 = -52.758620689655174\n",
      "Trend avg in 2009 = -45.977777777777774\n",
      "Trend avg in 2010 = -48.25806451612903\n",
      "Trend avg in 2011 = -49.90163934426229\n",
      "Trend avg in 2012 = -39.60344827586207\n",
      "Trend avg in 2013 = -54.049180327868854\n",
      "Trend avg in 2014 = -43.833333333333336\n",
      "Trend avg in 2015 = -48.95238095238095\n",
      "Trend avg in 2016 = -40.0\n",
      "Trend avg in 2017 = -47.16883116883117\n"
     ]
    }
   ],
   "source": [
    "for key,value in trends_per_year_avg_ord.items():\n",
    "    print( \"Trend avg in \" + str(key) + \" = \" + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deaths, Trend sum, Trend average in 2004 = 56, -2862, -51.107142857142854\n",
      "Deaths, Trend sum, Trend average in 2005 = 48, -2604, -54.25\n",
      "Deaths, Trend sum, Trend average in 2006 = 56, -3329, -59.44642857142857\n",
      "Deaths, Trend sum, Trend average in 2007 = 48, -2884, -60.083333333333336\n",
      "Deaths, Trend sum, Trend average in 2008 = 58, -3060, -52.758620689655174\n",
      "Deaths, Trend sum, Trend average in 2009 = 45, -2069, -45.977777777777774\n",
      "Deaths, Trend sum, Trend average in 2010 = 62, -2992, -48.25806451612903\n",
      "Deaths, Trend sum, Trend average in 2011 = 61, -3044, -49.90163934426229\n",
      "Deaths, Trend sum, Trend average in 2012 = 58, -2297, -39.60344827586207\n",
      "Deaths, Trend sum, Trend average in 2013 = 61, -3297, -54.049180327868854\n",
      "Deaths, Trend sum, Trend average in 2014 = 60, -2630, -43.833333333333336\n",
      "Deaths, Trend sum, Trend average in 2015 = 63, -3084, -48.95238095238095\n",
      "Deaths, Trend sum, Trend average in 2016 = 78, -3120, -40.0\n",
      "Deaths, Trend sum, Trend average in 2017 = 77, -3632, -47.16883116883117\n"
     ]
    }
   ],
   "source": [
    "for key,value in trends_per_year_avg_ord.items():\n",
    "    print( \"Deaths, Trend sum, Trend average in \" + str(key) + \" = \" + str(deaths_per_year[key]) + \", \" + str(trends_per_year_sum[key]) + \", \" + str(trends_per_year_avg[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of positive trend deaths per year\n",
    "positive_deaths_per_year = defaultdict(int)\n",
    "for death,trend in zip(df_valid[\"Died\"],df_valid[\"Trends\"]):\n",
    "    if trend >= 0:\n",
    "        positive_deaths_per_year[death.year] += 1\n",
    "positive_deaths_sorted = OrderedDict(sorted(positive_deaths_per_year.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Trend Deaths in 2004 = 5\n",
      "Positive Trend Deaths in 2005 = 3\n",
      "Positive Trend Deaths in 2006 = 1\n",
      "Positive Trend Deaths in 2008 = 4\n",
      "Positive Trend Deaths in 2009 = 4\n",
      "Positive Trend Deaths in 2010 = 7\n",
      "Positive Trend Deaths in 2011 = 5\n",
      "Positive Trend Deaths in 2012 = 8\n",
      "Positive Trend Deaths in 2013 = 2\n",
      "Positive Trend Deaths in 2014 = 8\n",
      "Positive Trend Deaths in 2015 = 8\n",
      "Positive Trend Deaths in 2016 = 11\n",
      "Positive Trend Deaths in 2017 = 9\n"
     ]
    }
   ],
   "source": [
    "for key,value in positive_deaths_sorted.items():\n",
    "    print( \"Positive Trend Deaths in \" + str(key) + \" = \" + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
